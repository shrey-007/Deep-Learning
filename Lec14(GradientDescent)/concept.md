### **Gradient Descent Variants**

Gradient Descent (GD) is an optimization algorithm used to update model parameters by minimizing the loss function. There are three main types:

| Type                     | Update Frequency | Pros âœ… | Cons âŒ |
|--------------------------|-----------------|--------|--------|
| **Batch Gradient Descent (BGD)** | After processing the **entire dataset** | Stable, converges smoothly | Slow for large datasets |
| **Stochastic Gradient Descent (SGD)** | After **each training sample** | Faster updates, good for online learning | Noisy convergence |
| **Mini-Batch Gradient Descent (MBGD)** | After processing a **small batch** of samples | Balance between BGD and SGD, efficient | Needs batch tuning |

---

### **1ï¸âƒ£ Batch Gradient Descent (BGD)**
- Uses **all training samples** to compute the gradient.
- **Pros**:  
  âœ” More stable updates, less noisy.  
  âœ” Guaranteed to reach the minimum (for convex functions).
- **Cons**:  
  âŒ Computationally expensive for large datasets.  
  âŒ Requires loading all data into memory.

ğŸš€ **Used in small datasets where memory is not a concern.**

---

### **2ï¸âƒ£ Stochastic Gradient Descent (SGD)**
- Updates the model **after each training example**.
- **Pros**:  
  âœ” Faster updates, allows **real-time learning**.  
  âœ” Good for **online learning** and streaming data.
- **Cons**:  
  âŒ Updates are noisy, causing fluctuations.  
  âŒ May not converge smoothly.

ğŸš€ **Used for large datasets, online learning, reinforcement learning.**

---

### **3ï¸âƒ£ Mini-Batch Gradient Descent (MBGD)**
- Uses a **subset (batch) of data** to compute gradients.
- **Pros**:  
  âœ” Faster than BGD, more stable than SGD.  
  âœ” Efficient with **GPU acceleration**.
- **Cons**:  
  âŒ Needs proper batch size selection.  
  âŒ Still requires memory for batches.

ğŸš€ **Most commonly used in deep learning (e.g., TensorFlow, PyTorch).**

---

### **Comparison Summary**

| Method | Stability | Speed | Memory | Best For |
|--------|----------|-------|--------|----------|
| **Batch GD** | High âœ… | Slow âŒ | High âŒ | Small datasets |
| **SGD** | Noisy âŒ | Fast âœ… | Low âœ… | Large datasets, online learning |
| **Mini-Batch GD** | Balanced âœ… | Fast âœ… | Medium âœ… | Deep learning |

---

### **Which One to Use?**
ğŸ”¹ **Small datasets** â†’ **Batch GD**  
ğŸ”¹ **Large datasets / Online Learning** â†’ **SGD**  
ğŸ”¹ **Deep Learning (most cases)** â†’ **Mini-Batch GD**

Would you like an example implementation in Java? ğŸš€